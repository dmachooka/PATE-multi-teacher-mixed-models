{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce4miP-4n0qk"
      },
      "source": [
        "# Differential Privacy for BotnetIot Dataset\n",
        "*Project Background:* <br>\n",
        "We, as a student, has a labeled private dataset and an unlabeled public dataset. We would like to collaberate with certain number of teachers to label our public dataset by training their similar datasets. In order to protect the data privacy of the teachers, we agree to add global noise to the labels they return for the public dataset. Then we label our public dataset based on the labels that most teachers agree. We combine the new labels with the public dataset and train with our private dataset. <br>\n",
        "\n",
        "*PATE analysis* is performed on teachers' predictions and the new labels with certain level of epsilon indicating how much information leakage is allowed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rLY1dsO-QfW",
        "outputId": "ad4c6be2-a35e-48f9-e4ea-548709978a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.12/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn) (0.14.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv4rJtShKWC6"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install syft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpCdyDIok8oZ"
      },
      "outputs": [],
      "source": [
        "# import necessary packages\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "# from syft.frameworks.torch.differential_privacy import pate\n",
        "# import syft as sy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import resample, shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc\n",
        "from sklearn.metrics import (\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        "     auc,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#Filtering errors\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1793b79e",
        "outputId": "68649bdc-6c02-4a81-c04e-b769b19e301d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.12/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn) (0.14.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install imblearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUytC-A-m2Zg"
      },
      "source": [
        "### Section 1: Download and load MNIST dataset\n",
        "\n",
        "*   **Trainset**: divide among teachers without overlapping\n",
        "*   **Testset**: split into private(labeled) and public(unlabeled) datasets; use models trained by teachers to label public dataset and train with the private dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYMQAOoZjMdP",
        "outputId": "ad2f9aab-3826-4835-b65c-f71aa83ec0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data  = pd.read_csv('/content/drive/MyDrive/BotNeTIoT-L01_label_NoDuplicates.csv', index_col = 0)\n",
        "target_column = 'label'\n",
        "\n",
        "num_classes = data[target_column].nunique()\n",
        "\n",
        "# Add Upsampling\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Perform upsampling (for synthetic minority over sampling() smote)\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "# put it in one area rebuild dataset\n",
        "data = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# # Label encode the target column(convert text to numbers)\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Instantiate LabelEncoder\n",
        "# label_encoder = LabelEncoder()\n",
        "\n",
        "# # Encode the target column\n",
        "# data[target_column] = label_encoder.fit_transform(data[target_column])\n",
        "\n",
        "# # Identify non-numeric columns (only two so dropping)\n",
        "# print(data.select_dtypes(include='object').columns)\n",
        "\n",
        "# data.drop(['proto', 'service'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckTIiYM5mcjr"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "y = data[target_column].values\n",
        "X = data.drop(columns=[target_column]).values  #\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Define a custom dataset\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "trainset = TabularDataset(X_train, y_train)\n",
        "testset = TabularDataset(X_test, y_test)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71jTLYE8mr-n",
        "outputId": "204bcc78-6b0e-4197-c508-e24a37403ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num training data: 3252230\n",
            "num test data: 573924\n"
          ]
        }
      ],
      "source": [
        "# training and test data info\n",
        "print('num training data: {0}'.format(len(trainset)))\n",
        "print('num test data: {0}'.format(len(testset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yx9_2Y5og5C"
      },
      "source": [
        "### Section 2: Functions\n",
        "\n",
        "\n",
        "*   `teacher_data_allocator`: evenly allocate training dataset to the number of teachers as their private datasets\n",
        "*   `private_public_allocator`: split test dataset as the student's private dataset and public dataset\n",
        "*   `train`: model training with training and valid datasets\n",
        "*   `get_label_testset`: get labels of the student's public dataset from all teachers\n",
        "*   `add_global_noise`: add global noise to the labels that most teachers agree\n",
        "*   `create_labeled_public_db`: extract images from the public dataset and combine these images with the new labels; create a new dataloader for this new dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laCE8KjVpfyv"
      },
      "outputs": [],
      "source": [
        "# divide unique training datasets among teachers\n",
        "# split each dataset into training and valid sets\n",
        "def teacher_data_allocator(trainset, batch_size=64, num_teachers=10, valid_train_split=0.3):\n",
        "  data_per_teacher = len(trainset) // num_teachers    # round to the nearest integer\n",
        "  train_loaders = []\n",
        "  valid_loaders = []\n",
        "  for i in range(num_teachers):\n",
        "    if i == num_teachers - 1:\n",
        "      ind = list(range(i * data_per_teacher, len(trainset)))\n",
        "    else:\n",
        "      ind = list(range(i * data_per_teacher, (i + 1) * data_per_teacher))\n",
        "    data = Subset(trainset, ind)\n",
        "    split = int(valid_train_split * len(data))\n",
        "    valid_data, train_data = random_split(data, [split, len(data) - split])\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
        "    train_loaders.append(train_loader)\n",
        "    valid_loaders.append(valid_loader)\n",
        "  return train_loaders, valid_loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO2s8OsywWDE"
      },
      "outputs": [],
      "source": [
        "# divide testset into private and public\n",
        "def private_public_allocator(dataset, batch_size=64, split=0.7):\n",
        "  split_ind = int(len(dataset) * split)\n",
        "\n",
        "  # validset and testset\n",
        "  # validset is smaller because dataset of each teacher is small\n",
        "  private_ind = list(range(0, split_ind))\n",
        "  public_ind = list(range(split_ind, len(dataset)))\n",
        "  private_set = Subset(dataset, private_ind)\n",
        "  public_set = Subset(dataset, public_ind)\n",
        "\n",
        "  # validloader and testloader\n",
        "  private_loader = torch.utils.data.DataLoader(private_set, batch_size=batch_size, shuffle=True)\n",
        "  public_loader = torch.utils.data.DataLoader(public_set, batch_size=batch_size, shuffle=False)\n",
        "  return private_loader, public_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHv26UDqE14c"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, trainloader, validloader, epochs=30, patience = 5, plot=False):\n",
        "    model = model\n",
        "    criterion = criterion\n",
        "    optimizer = optimizer\n",
        "    epochs = epochs\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accuracies, valid_accuracies = [], []\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_valid_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    early_stop = False\n",
        "\n",
        "    for e in range(epochs):\n",
        "        if early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        running_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            log_ps = model(inputs)\n",
        "            loss = criterion(log_ps, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Track accuracy for training\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            correct_train += equals.sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Validation loop\n",
        "        valid_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for inputs, labels in validloader:\n",
        "                log_ps = model(inputs)\n",
        "                valid_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                correct_val += equals.sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "        valid_losses.append(valid_loss / len(validloader))\n",
        "        train_accuracies.append(correct_train / total_train)\n",
        "        valid_accuracies.append(correct_val / total_val)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch: {e+1}/{epochs}.. \"\n",
        "              f\"Training Loss: {running_loss / len(trainloader):.3f}.. \"\n",
        "              f\"Validation Loss: {valid_loss / len(validloader):.3f}.. \"\n",
        "              f\"Validation Accuracy: {correct_val / total_val:.3f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        current_valid_loss = valid_loss / len(validloader)\n",
        "        if current_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = current_valid_loss\n",
        "            epochs_no_improve = 0  # Reset counter\n",
        "            best_model_state = model.state_dict()  # Save best model\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Validation loss has not improved for {patience} epochs. Stopping early.\")\n",
        "            early_stop = True\n",
        "\n",
        "    # Load the best model state before returning\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    if plot:\n",
        "        return model, train_losses, valid_losses, train_accuracies, valid_accuracies\n",
        "    else:\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TZYgHy2GVsR"
      },
      "outputs": [],
      "source": [
        "# label testset by models\n",
        "def get_label_testset(models, testloader):\n",
        "  test_labels = []\n",
        "  for model in models:\n",
        "    test_label = []\n",
        "    for inputs, _ in testloader:\n",
        "      with torch.no_grad():\n",
        "        log_ps = model(inputs)\n",
        "        ps = torch.exp(log_ps)\n",
        "      top_p, top_class = ps.topk(1, dim=1)\n",
        "      test_label.append(top_class.squeeze().tolist())\n",
        "    test_label = sum(test_label, [])\n",
        "    test_labels.append(test_label)\n",
        "  return test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxmgZtRAC4fO"
      },
      "outputs": [],
      "source": [
        "# get predictions with global noise\n",
        "def add_global_noise(preds, epsilon=0.1):\n",
        "  labels_with_noise = []\n",
        "  for pred in preds:    # go thru entries\n",
        "    label_counts = np.bincount(pred, minlength=preds.shape[1])\n",
        "\n",
        "    epsilon = epsilon\n",
        "    beta = 1 / epsilon\n",
        "\n",
        "    for i in range(len(label_counts)):\n",
        "      label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    new_label = np.argmax(label_counts) % num_classes\n",
        "    labels_with_noise.append(new_label)\n",
        "  return np.array(labels_with_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdzQ8GZre3UJ"
      },
      "outputs": [],
      "source": [
        "# create a new dataloader for the public dataset\n",
        "# using the new labels added global noise\n",
        "def create_labeled_public_db(dataloader, labels, batch_size=64):\n",
        "  image_list = []\n",
        "  for image, _ in dataloader:\n",
        "    image_list.append(image)\n",
        "  data = np.vstack(image_list)\n",
        "  new_dataloader = list(zip(data, labels))\n",
        "  new_dataloader = torch.utils.data.DataLoader(new_dataloader, shuffle=False, batch_size=batch_size)\n",
        "  return new_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7op5yp7Xsx"
      },
      "source": [
        "### Section 3: Define neural network model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA7zKRJb1IGM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "\n",
        "# Teacher 1: LSTM\n",
        "class TeacherLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        out = self.fc(hn[-1])\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "# Teacher 2: MLP (your current one)\n",
        "class TeacherMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        out = self.fc3(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "# Teacher 3: CNN\n",
        "class TeacherCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes=2):\n",
        "        super().__init__()\n",
        "        # treat each feature vector as [channels=1, length=input_size]\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [batch, features] -> [batch, 1, features]\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)          # flatten all except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        out = self.fc2(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJasB4dIlats"
      },
      "outputs": [],
      "source": [
        "class StudentGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        _, hn = self.gru(x)\n",
        "        out = self.fc(hn[-1])\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "class StudentCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes=2, num_channels=64, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_channels, kernel_size=kernel_size, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(num_channels)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels * 2, kernel_size=kernel_size, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_channels * 2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(num_channels * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If input is [batch, seq_len, features], permute for Conv1d: [batch, features, seq_len]\n",
        "        if x.dim() == 3:\n",
        "            x = x.permute(0, 2, 1)\n",
        "        elif x.dim() == 2:\n",
        "            x = x.unsqueeze(2)  # [batch, features, 1]\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.global_pool(x).squeeze(-1)  # [batch, channels]\n",
        "        out = self.fc(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "class StudentMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_dim = input_size\n",
        "\n",
        "        # Build hidden layers dynamically\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(nn.Linear(in_dim, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            in_dim = hidden_size\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If input is [batch, seq_len, features], flatten seq_len dimension\n",
        "        if x.dim() == 3:\n",
        "            x = x.mean(dim=1)  # average pooling over time dimension\n",
        "        out = self.mlp(x)\n",
        "        out = self.fc(out)\n",
        "        return F.log_softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezGTqiDvC9xV"
      },
      "source": [
        "### Section 4: Model training and PATE analysis\n",
        "We first ask the teachers to train their data and use these models to label the student's public data. After adding the global noise to create the new labels for the public dataset, the student then trains his/her private (training set) and public (valid set) data. PATE analysis returns independent and dependent epsilons show how much information has been leaked and how much teachers agree with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrJ8gF33CpXi",
        "outputId": "81f5f5e0-deac-4ed6-ecd9-a77700b7ac14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "# allocate training dataset to teachers\n",
        "# split each of these datasets into training and valid datasets\n",
        "teachers_train, teachers_valid = teacher_data_allocator(trainset)\n",
        "print(len(teachers_train))\n",
        "print(len(teachers_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuQI9wMoCjVd",
        "outputId": "553c0805-db2b-4188-ffc6-a36f0f786a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6278\n",
            "2691\n"
          ]
        }
      ],
      "source": [
        "# allocate test dataset to the student\n",
        "# split the dataset into private and public datasets\n",
        "private_loader, public_loader = private_public_allocator(testset)\n",
        "print(len(private_loader))\n",
        "print(len(public_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXMsrC6WGctw",
        "outputId": "6cc020b4-8735-4f22-92eb-b2b02d12e2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training Loss: 0.004.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 2/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 3/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 4/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 5/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 6/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 7/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 8/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 9/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 10/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 11/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 12/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 13/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Validation loss has not improved for 5 epochs. Stopping early.\n",
            "Early stopping triggered\n",
            "Epoch: 1/30.. Training Loss: 0.004.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 2/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 3/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 4/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 5/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 6/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 7/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 8/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 9/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 10/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 11/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 12/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 13/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 14/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 15/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 16/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 17/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 18/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 19/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 20/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 21/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Validation loss has not improved for 5 epochs. Stopping early.\n",
            "Early stopping triggered\n",
            "Epoch: 1/30.. Training Loss: 0.004.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 2/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 3/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 4/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 5/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 6/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 7/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 8/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 9/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 10/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 11/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 12/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 13/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 14/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 15/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 16/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Epoch: 17/30.. Training Loss: 0.001.. Validation Loss: 0.001.. Validation Accuracy: 1.000\n",
            "Validation loss has not improved for 5 epochs. Stopping early.\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-269576571.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  label_counts[i] += np.random.laplace(0, beta, 1)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "\n",
        "# Initialize teachers\n",
        "teacher1 = TeacherMLP(input_size=input_size, num_classes=num_classes)\n",
        "teacher2 = TeacherMLP(input_size=input_size, num_classes=num_classes)\n",
        "teacher3 = TeacherMLP(input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "teacher4 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "teacher5 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "teacher6 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "teacher7 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "teacher8 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "teacher9 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "teacher10 = TeacherCNN(input_size=input_size, num_classes=num_classes)\n",
        "\n",
        "teachers = [teacher1, teacher2, teacher3] # ,teacher4, teacher5, teacher6,teacher7, teacher8, teacher9, teacher10]\n",
        "\n",
        "# Loss and optimizer for teachers\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_models = []\n",
        "for t in teachers:\n",
        "    optimizer = optim.Adam(t.parameters(), lr=0.0001)\n",
        "    teacher_model = train(t, criterion, optimizer, train_loader, test_loader, epochs=30)\n",
        "    teacher_models.append(teacher_model)\n",
        "\n",
        "\n",
        "# Teachers label the public dataset\n",
        "teacher_preds = get_label_testset(teachers, public_loader)\n",
        "\n",
        "# Aggregate with global noise\n",
        "noisy_labels = add_global_noise(np.array(teacher_preds), epsilon=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FuUFTUtyTrDT"
      },
      "outputs": [],
      "source": [
        "# predict labels for testset from each teacher\n",
        "# row = num of test data\n",
        "# col = predictions from each teacher\n",
        "preds = get_label_testset(teacher_models, public_loader)\n",
        "preds = np.array([np.array(p) for p in preds]).transpose(1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mAGGbiy9pV7e",
        "outputId": "f4210546-e914-401f-f74b-8d36a34a16ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-269576571.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  label_counts[i] += np.random.laplace(0, beta, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "172178"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_with_noise = add_global_noise(preds, epsilon=1.3)\n",
        "len(preds_with_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PoGCUUNLJb9Q",
        "outputId": "a945fafc-1ea7-4a10-fde5-498d6ff8b997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data-independent epsilon: 0.7692307692307692\n",
            "Data-dependent epsilon: 8.033292351643122e-05\n"
          ]
        }
      ],
      "source": [
        "# PATE Analysis manual\n",
        "teacher_preds = preds\n",
        "noise_eps = 1.3 # Privacy noise epsilon (scale parameter of Laplace noise)\n",
        "delta = 1e-5 # Privacy delta (used in the privacy guarantee)\n",
        "\n",
        "\n",
        "\n",
        "# Compute max votes for each query\n",
        "max_votes = np.max(teacher_preds, axis=0) # Maximum votes for each query\n",
        "total_queries = teacher_preds.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "# Data-independent epsilon\n",
        "data_ind_eps = (1 / noise_eps) * np.max(max_votes)\n",
        "print(\"Data-independent epsilon:\", data_ind_eps)\n",
        "\n",
        "\n",
        "\n",
        "# Data-dependent epsilon\n",
        "total_votes = np.sum(teacher_preds, axis=0)\n",
        "data_dep_eps = np.sum(\n",
        "np.log(1 + (total_queries * max_votes) / (noise_eps * total_votes))\n",
        ")\n",
        "print(\"Data-dependent epsilon:\", data_dep_eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "nGujLbX3p-Rc"
      },
      "outputs": [],
      "source": [
        "# !pip install syft\n",
        "# import syft as sy\n",
        "# # PATE analysis\n",
        "# data_dep_eps, data_ind_eps = sy.pate.perform_analysis(teacher_preds=preds.T, indices=preds_with_noise, noise_eps=0.1, delta=1e-5)\n",
        "# print('Data dependent epsilon:', data_dep_eps)\n",
        "# print('Data independent epsilon:', data_ind_eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ifLpJDqlNcVS",
        "outputId": "82ea6f2c-3ea6-426c-c12c-5636dd801b0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2691"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create new dataloader for the public data\n",
        "# using the original function\n",
        "public_loader_labeled = create_labeled_public_db(public_loader, preds_with_noise)\n",
        "len(public_loader_labeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tENIpozxYkhe"
      },
      "outputs": [],
      "source": [
        "# train model using private and public datasets\n",
        "my_model = StudentMLP(input_size=input_size, num_classes=num_classes)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(my_model.parameters(), lr=0.0001)\n",
        "\n",
        "my_model, train_losses, valid_losses, train_accuracies, valid_accuracies = train(\n",
        "    model=my_model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    trainloader=private_loader,\n",
        "    validloader=public_loader_labeled,\n",
        "    epochs=30, plot = True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP314ZhQpgxv"
      },
      "source": [
        "## Student Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si-FQSMRoJO8"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Training and Validation Losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(range(1, len(valid_losses)+1), valid_losses, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Training Accuracy', color='blue')\n",
        "plt.plot(range(1, len(valid_accuracies)+1), valid_accuracies, label='Validation Accuracy', color='red')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yscbaLQhwjnc"
      },
      "outputs": [],
      "source": [
        "print(valid_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVHhCkiBqg6h"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(model, validloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validloader:\n",
        "            log_ps = model(inputs)\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            all_preds.extend(top_class.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# Get predictions and true labels\n",
        "all_preds, all_labels = generate_predictions(my_model, public_loader_labeled)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Print F1 score\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot confusion matrix with class names\n",
        "class_labels = data[target_column].unique().tolist() #label_encoder.classes_\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXCk3TL6rt51"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}